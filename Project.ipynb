{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJaY03Bvr4mYrLk4Gp3p3z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aparimita18/Capstone-project/blob/main/Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uBw0DCZbObT"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Model  \n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, GlobalAveragePooling2D, Input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Flatten\n",
        "\n",
        "# Supress info, warnings and error messages\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect Google Drive with Google Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Hcq87RL8cFes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect Dataset and assemble the data in tabular form\n",
        "\n",
        "disease_types = ['COVID', 'non-COVID']\n",
        "\n",
        "train_dir = data_dir = '/content/drive/My Drive/Colab Notebooks/Dataset'\n",
        "\n",
        "train_data = []\n",
        "\n",
        "for index, sp in enumerate(disease_types):\n",
        "    for file in os.listdir(os.path.join(train_dir, sp)):\n",
        "        train_data.append([sp + \"/\" + file, index, sp])\n",
        "        \n",
        "train = pd.DataFrame(train_data, columns = ['File', 'ID','Disease Type'])\n",
        "train"
      ],
      "metadata": {
        "id": "_hvtZdDycQ3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Seed = 40\n",
        "\n",
        "train = train.sample(frac = 1, replace=False, random_state = Seed)\n",
        "\n",
        "# Reset indices (row numbers)\n",
        "train = train.reset_index(drop = True)\n",
        "\n",
        "sns.countplot(x = \"ID\", data = train).set_title(\"Frequency Histogram (0: COVID, 1:Non-COVID)\")\n",
        "train"
      ],
      "metadata": {
        "id": "mavm8VzScg1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_defects(defect_types, rows, cols):\n",
        "    fig, ax = plt.subplots(rows, cols, figsize=(12, 12))\n",
        "    defect_files = train['File'][train['Disease Type'] == defect_types].values\n",
        "    \n",
        "    n = 0\n",
        "    fig.suptitle(defect_types, fontsize = 22, color = \"white\")\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            image_path = os.path.join(data_dir, defect_files[n])\n",
        "            ax[i, j].set_xticks([])\n",
        "            ax[i, j].set_yticks([])\n",
        "            ax[i, j].imshow(cv2.imread(image_path))\n",
        "            n += 1\n",
        "\n",
        "\n",
        "plot_defects('COVID', 3, 3)\n",
        "plot_defects('non-COVID', 3, 3)"
      ],
      "metadata": {
        "id": "FxV_OyLscl67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 224\n",
        "\n",
        "# OpenCV Function to load colored image\n",
        "def read_image(filepath):\n",
        "    return cv2.imread(os.path.join(data_dir, filepath))\n",
        "\n",
        "# OpenCV Function to resize an image\n",
        "def resize_image(image, image_size):\n",
        "    return cv2.resize(image.copy(), image_size, interpolation = cv2.INTER_AREA)"
      ],
      "metadata": {
        "id": "06x3RvA7cpaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the data\n",
        "\n",
        "X_train = np.zeros((train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "\n",
        "for i, file in enumerate(train['File'].values):\n",
        "    image = read_image(file)\n",
        "    if image is not None:\n",
        "        X_train[i] = resize_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "\n",
        "X_Train = X_train / 255.0   # Pixel normalization\n",
        "print('Train Shape:', X_Train.shape)\n",
        "\n",
        "Y_train = to_categorical(train['ID'].values, num_classes = 2)\n",
        "\n",
        "print(Y_train)"
      ],
      "metadata": {
        "id": "M9Q0iYAjcvwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe split to train and validation set (80% train and 20% validation)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_Train,\n",
        "                                                  Y_train,\n",
        "                                                  test_size = 0.2, # Percent 20% of the data is using as test set\n",
        "                                                  random_state = Seed)\n",
        "\n",
        "print(f'X_train:', X_train.shape)\n",
        "print(f'X_val:', X_val.shape)\n",
        "print(f'Y_train:', Y_train.shape)\n",
        "print(f'Y_val:', Y_val.shape)"
      ],
      "metadata": {
        "id": "6IyowoqpczSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Architectural function for Resnet50\n",
        "def build_resnet50(IMAGE_SIZE, channels):\n",
        "\n",
        "    resnet50 = ResNet50(weights = 'imagenet', include_top = False)\n",
        "\n",
        "    input = Input(shape = (IMAGE_SIZE, IMAGE_SIZE, channels))\n",
        "    x = Conv2D(3, (3, 3), padding = 'same')(input)\n",
        "    x = resnet50(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(64, activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    output = Dense(2, activation = 'softmax')(x)\n",
        " \n",
        "    # model\n",
        "    model = Model(input, output)\n",
        "    \n",
        "    optimizer = Adam(learning_rate = 0.003, beta_1 = 0.9, beta_2 = 0.999, epsilon = 0.1)\n",
        "    model.compile(loss = 'categorical_crossentropy',  # minimize the negative multinomial log-likelihood also known as the cross-entropy.\n",
        "                  optimizer = optimizer,\n",
        "                  metrics = ['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "channels = 3\n",
        "\n",
        "model = build_resnet50(IMAGE_SIZE, channels)\n",
        "annealer = ReduceLROnPlateau(monitor = 'val_accuracy',  # Reduce learning rate when Validation accuracy remains constant\n",
        "                             factor = 0.70,  # Rate by which the learning rate will decrease\n",
        "                             patience = 5,   # number of epochs without improvement, after which the learning rate will decrease\n",
        "                             verbose = 1,    # Display messages\n",
        "                             min_lr = 1e-4   # lower limit on the learning rate.\n",
        "                            )\n",
        "checkpoint = ModelCheckpoint('model.h5', verbose = 1, save_best_only = True)  # Save neural network weights\n",
        "\n",
        "# Generates batches of image data with data augmentation\n",
        "datagen = ImageDataGenerator(rotation_range = 360, # Degree range for random rotations\n",
        "                        width_shift_range = 0.2,   # Range for random horizontal shifts\n",
        "                        height_shift_range = 0.2,  # Range for random vertical shifts\n",
        "                        zoom_range = 0.2,          # Range for random zoom\n",
        "                        horizontal_flip = True,    # Randomly flip inputs horizontally\n",
        "                        vertical_flip = True)      # Randomly flip inputs vertically\n",
        "\n",
        "datagen.fit(X_train)\n",
        "\n",
        "plot_model(model, to_file = 'convnet.png', show_shapes = True, show_layer_names = True)"
      ],
      "metadata": {
        "id": "jc7yRWjgd_-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 15\n",
        "EPOCHS = 5\n",
        "\n",
        "# Fit of the model that includes the augmented images in terms of their characteristics\n",
        "hist = model.fit(datagen.flow(X_train, Y_train, batch_size = BATCH_SIZE),\n",
        "               steps_per_epoch = X_train.shape[0] // BATCH_SIZE,\n",
        "               epochs = EPOCHS,\n",
        "               verbose = 1,\n",
        "               callbacks = [annealer, checkpoint],\n",
        "               validation_data = (X_val, Y_val))"
      ],
      "metadata": {
        "id": "YQeHz-rReP3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check all performance metrices\n",
        "\n",
        "Y_pred = model.predict(X_val)\n",
        "\n",
        "Y_pred = np.argmax(Y_pred, axis = 1)\n",
        "Y_true = np.argmax(Y_val, axis = 1)\n",
        "\n",
        "cm = confusion_matrix(Y_true, Y_pred)\n",
        "plt.figure(figsize = (12, 12))\n",
        "ax = sns.heatmap(cm, cmap = plt.cm.Greens, annot = True, square = True, xticklabels = disease_types, yticklabels = disease_types)\n",
        "ax.set_ylabel('Actual', fontsize = 40)\n",
        "ax.set_xlabel('Predicted', fontsize = 40)\n",
        "\n",
        "\n",
        "TP = cm[1][1]\n",
        "print(f\"True Positive: {TP}\")\n",
        "\n",
        "\n",
        "FN = cm[1][0]\n",
        "print(f\"False Negative: {FN}\")\n",
        "\n",
        "TN = cm[0][0]\n",
        "print(f\"True Negative: {TN}\")\n",
        "\n",
        "FP = cm[0][1]\n",
        "print(f\"False Positive: {FP}\")\n",
        "\n",
        "# Sensitivity, recall, or true positive rate\n",
        "print(f\"True Positive Rate: {TP / (TP + FN)}\")\n",
        "\n",
        "# Specificity or true negative rate\n",
        "print(f\"True Negative Rate: {TN / (TN + FP)}\\n\")\n",
        "\n",
        "final_loss, final_accuracy = model.evaluate(X_val, Y_val)\n",
        "print(f\"\\nFinal Loss: {final_loss}, Final Accuracy: {final_accuracy}\")"
      ],
      "metadata": {
        "id": "tfyZS7cMebr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy plot \n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc = 'upper left')\n",
        "plt.show()\n",
        "\n",
        "# Loss plot\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc = 'upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jRv1awkPDvJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Final Output Prediction\n",
        "\n",
        "from keras.preprocessing import image\n",
        "\n",
        "img = image.load_img('/content/drive/My Drive/CT scan/COVID/Covid (1007).png', grayscale = False, target_size = (224, 224))\n",
        "show_img = image.load_img('/content/drive/My Drive/CT scan/COVID/Covid (1007).png', grayscale = False, target_size = (200, 200))\n",
        "disease_class = ['Covid-19','Non Covid-19']\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis = 0)\n",
        "x /= 255\n",
        "\n",
        "custom = model.predict(x)\n",
        "print(custom[0])\n",
        "\n",
        "plt.imshow(show_img)\n",
        "plt.show()\n",
        "\n",
        "a = custom[0]\n",
        "ind = np.argmax(a)\n",
        "        \n",
        "print('Prediction:',disease_class[ind])"
      ],
      "metadata": {
        "id": "FNNKqGqPDy-7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
